{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtensionLive.png  front  kernel.ipynb\tLICENSE  mle  README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [],
   "source": [
    "\n",
    "nyt = 'mle/nyt.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing, string\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# def tokenize(text):\n",
    "#     tokens = []; word = \"\"\n",
    "#     for char in text.lower():\n",
    "#         if (char in string.whitespace) or (char in string.punctuation):\n",
    "#             if word:\n",
    "#                 tokens.append(word.strip(' '))\n",
    "#                 word = \"\"\n",
    "#         if char in string.punctuation:\n",
    "#             tokens.append(char)\n",
    "#         else:\n",
    "#             word += char\n",
    "#     return [word for word in tokens]\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', '')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    #tokens = tokenize(doc)\n",
    "    # remove punctuation from each token\n",
    "    #table = str.maketrans('', '', string.punctuation)\n",
    "    #tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    #tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    #tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheryl Sandberg was seething.\n",
      "\n",
      "Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, m\n",
      "['Sheryl', 'Sandberg', 'was', 'seething.', 'Inside', 'Facebook’s', 'Menlo', 'Park,', 'Calif.,', 'headquarters,', 'top', 'executives', 'gathered', 'in', 'the', 'glass-walled', 'conference', 'room', 'of', 'its', 'founder,', 'Mark', 'Zuckerberg.', 'It', 'was', 'September', '2017,', 'more', 'than', 'a', 'year', 'after', 'Facebook', 'engineers', 'discovered', 'suspicious', 'Russia-linked', 'activity', 'on', 'its', 'site,', 'an', 'early', 'warning', 'of', 'the', 'Kremlin', 'campaign', 'to', 'disrupt']\n",
      "Total Tokens: 5269\n",
      "Unique Tokens: 2016\n"
     ]
    }
   ],
   "source": [
    "# load document\n",
    "#in_filename = republic\n",
    "in_filename = nyt\n",
    "doc = load_doc(in_filename)\n",
    "print(doc[:200])\n",
    "\n",
    "# clean document\n",
    "tokens = clean_doc(doc)\n",
    "print(tokens[:50])\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5219\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of tokens\n",
    "length = 50\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    # select sequence of tokens\n",
    "    seq = tokens[i-length:i]\n",
    "    # convert into a line\n",
    "    line = ' '.join(seq)\n",
    "    # store\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [],
   "source": [
    "# save sequences to file\n",
    "#out_filename = 'republic_sequences.txt'\n",
    "out_filename = 'mle/sequences.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [],
   "source": [
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "55969077747ed79521f91655c77b8560a42cd59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheryl Sandberg was seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt\r\n",
      "Sandberg was seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt the\r\n"
     ]
    }
   ],
   "source": [
    "!head sequences.txt -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Word rNN (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jared/anaconda3/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Lambda\n",
    "from keras import backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sheryl Sandberg was seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt',\n",
       " 'Sandberg was seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt the',\n",
       " 'was seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt the 2016',\n",
       " 'seething. Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt the 2016 American',\n",
       " 'Inside Facebook’s Menlo Park, Calif., headquarters, top executives gathered in the glass-walled conference room of its founder, Mark Zuckerberg. It was September 2017, more than a year after Facebook engineers discovered suspicious Russia-linked activity on its site, an early warning of the Kremlin campaign to disrupt the 2016 American election.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load\n",
    "in_filename = 'sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(filters='')\n",
    "tokenizer.fit_on_texts(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5219, 50)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f2b1f41a6423cc1ba2a71494dcbbad7bef1148a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "764b2530b70191232c943a089164cc2d77171dfd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[0]), len(sequences[1]),len(sequences[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5219, 49)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 49, 50)            96800     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 49, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1936)              195536    \n",
      "=================================================================\n",
      "Total params: 443,236\n",
      "Trainable params: 443,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 7.1264 - acc: 0.0374\n",
      "Epoch 2/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.5557 - acc: 0.0519\n",
      "Epoch 3/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.4769 - acc: 0.0519\n",
      "Epoch 4/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.4597 - acc: 0.0519\n",
      "Epoch 5/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.4460 - acc: 0.0519\n",
      "Epoch 6/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.4091 - acc: 0.0519\n",
      "Epoch 7/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 6.2771 - acc: 0.0519\n",
      "Epoch 8/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 6.1233 - acc: 0.0535\n",
      "Epoch 9/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 5.9828 - acc: 0.0548\n",
      "Epoch 10/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 5.8753 - acc: 0.0561\n",
      "Epoch 11/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 5.7953 - acc: 0.0548\n",
      "Epoch 12/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 5.7283 - acc: 0.0575\n",
      "Epoch 13/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 5.6732 - acc: 0.0592\n",
      "Epoch 14/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 5.6076 - acc: 0.0617\n",
      "Epoch 15/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 5.5270 - acc: 0.0602\n",
      "Epoch 16/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 5.4552 - acc: 0.0586\n",
      "Epoch 17/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 5.3904 - acc: 0.0628\n",
      "Epoch 18/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 5.3349 - acc: 0.0669\n",
      "Epoch 19/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 5.2790 - acc: 0.0659\n",
      "Epoch 20/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 5.2292 - acc: 0.0665\n",
      "Epoch 21/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 5.1775 - acc: 0.0705\n",
      "Epoch 22/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 5.1284 - acc: 0.0743\n",
      "Epoch 23/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 5.0782 - acc: 0.0751\n",
      "Epoch 24/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 5.0281 - acc: 0.0816\n",
      "Epoch 25/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 4.9763 - acc: 0.0822\n",
      "Epoch 26/200\n",
      "5219/5219 [==============================] - 19s 4ms/step - loss: 4.9245 - acc: 0.0839\n",
      "Epoch 27/200\n",
      "5219/5219 [==============================] - 20s 4ms/step - loss: 4.8760 - acc: 0.0839\n",
      "Epoch 28/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.8315 - acc: 0.0889\n",
      "Epoch 29/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 4.7964 - acc: 0.0895\n",
      "Epoch 30/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 4.7490 - acc: 0.0937\n",
      "Epoch 31/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.7083 - acc: 0.0906\n",
      "Epoch 32/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 4.6650 - acc: 0.0981\n",
      "Epoch 33/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 4.6210 - acc: 0.1006\n",
      "Epoch 34/200\n",
      "5219/5219 [==============================] - 21s 4ms/step - loss: 4.5827 - acc: 0.0981\n",
      "Epoch 35/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.5423 - acc: 0.1004\n",
      "Epoch 36/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 4.5017 - acc: 0.1006\n",
      "Epoch 37/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.4597 - acc: 0.1071\n",
      "Epoch 38/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 4.4199 - acc: 0.1065\n",
      "Epoch 39/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 4.3790 - acc: 0.1129\n",
      "Epoch 40/200\n",
      "5219/5219 [==============================] - 23s 5ms/step - loss: 4.3377 - acc: 0.1127\n",
      "Epoch 41/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.2967 - acc: 0.1100\n",
      "Epoch 42/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 4.2527 - acc: 0.1169\n",
      "Epoch 43/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.2147 - acc: 0.1207\n",
      "Epoch 44/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 4.1774 - acc: 0.1192\n",
      "Epoch 45/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 4.1352 - acc: 0.1236\n",
      "Epoch 46/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 4.0992 - acc: 0.1267\n",
      "Epoch 47/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 4.0534 - acc: 0.1265\n",
      "Epoch 48/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 4.0141 - acc: 0.1305\n",
      "Epoch 49/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.9844 - acc: 0.1311\n",
      "Epoch 50/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.9494 - acc: 0.1334\n",
      "Epoch 51/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.9110 - acc: 0.1416\n",
      "Epoch 52/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 3.8720 - acc: 0.1387\n",
      "Epoch 53/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 3.8444 - acc: 0.1399\n",
      "Epoch 54/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.8380 - acc: 0.1464\n",
      "Epoch 55/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 3.7906 - acc: 0.1404\n",
      "Epoch 56/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.7378 - acc: 0.1487\n",
      "Epoch 57/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.7085 - acc: 0.1521\n",
      "Epoch 58/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.6686 - acc: 0.1579\n",
      "Epoch 59/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.6358 - acc: 0.1565\n",
      "Epoch 60/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.5995 - acc: 0.1596\n",
      "Epoch 61/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.5643 - acc: 0.1686\n",
      "Epoch 62/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.5439 - acc: 0.1669\n",
      "Epoch 63/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.4943 - acc: 0.1701\n",
      "Epoch 64/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.4633 - acc: 0.1805\n",
      "Epoch 65/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.4234 - acc: 0.1782\n",
      "Epoch 66/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.3877 - acc: 0.1803\n",
      "Epoch 67/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 3.3643 - acc: 0.1845\n",
      "Epoch 68/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 3.3248 - acc: 0.1908\n",
      "Epoch 69/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.2862 - acc: 0.1937\n",
      "Epoch 70/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 3.2501 - acc: 0.2004\n",
      "Epoch 71/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 3.2184 - acc: 0.2014\n",
      "Epoch 72/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.1826 - acc: 0.2077\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5219/5219 [==============================] - 27s 5ms/step - loss: 3.1547 - acc: 0.2179\n",
      "Epoch 74/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 3.1206 - acc: 0.2180\n",
      "Epoch 75/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 3.0751 - acc: 0.2315\n",
      "Epoch 76/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 3.0401 - acc: 0.2330\n",
      "Epoch 77/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.0149 - acc: 0.2395\n",
      "Epoch 78/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 3.0032 - acc: 0.2376\n",
      "Epoch 79/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 2.9771 - acc: 0.2443\n",
      "Epoch 80/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 2.9310 - acc: 0.2466\n",
      "Epoch 81/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.9151 - acc: 0.2535\n",
      "Epoch 82/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.8728 - acc: 0.2568\n",
      "Epoch 83/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.8406 - acc: 0.2652\n",
      "Epoch 84/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.8036 - acc: 0.2717\n",
      "Epoch 85/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.7808 - acc: 0.2803\n",
      "Epoch 86/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.7659 - acc: 0.2734\n",
      "Epoch 87/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.7397 - acc: 0.2857\n",
      "Epoch 88/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.7095 - acc: 0.2884\n",
      "Epoch 89/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.6969 - acc: 0.2880\n",
      "Epoch 90/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.6512 - acc: 0.3035\n",
      "Epoch 91/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.6334 - acc: 0.2955\n",
      "Epoch 92/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.5909 - acc: 0.3169\n",
      "Epoch 93/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 2.5692 - acc: 0.3269\n",
      "Epoch 94/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 2.5463 - acc: 0.3273\n",
      "Epoch 95/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 2.5320 - acc: 0.3363\n",
      "Epoch 96/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 2.5088 - acc: 0.3309\n",
      "Epoch 97/200\n",
      "5219/5219 [==============================] - 29s 5ms/step - loss: 2.4824 - acc: 0.3388\n",
      "Epoch 98/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 2.4678 - acc: 0.3357\n",
      "Epoch 99/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.4348 - acc: 0.3505\n",
      "Epoch 100/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.4039 - acc: 0.3604\n",
      "Epoch 101/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.3782 - acc: 0.3671\n",
      "Epoch 102/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.3596 - acc: 0.3698\n",
      "Epoch 103/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.3323 - acc: 0.3807\n",
      "Epoch 104/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.3140 - acc: 0.3861\n",
      "Epoch 105/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.2920 - acc: 0.3922\n",
      "Epoch 106/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.2623 - acc: 0.3974\n",
      "Epoch 107/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.2619 - acc: 0.3897\n",
      "Epoch 108/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.2486 - acc: 0.3857\n",
      "Epoch 109/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.2064 - acc: 0.4070\n",
      "Epoch 110/200\n",
      "5219/5219 [==============================] - 22s 4ms/step - loss: 2.1715 - acc: 0.4148\n",
      "Epoch 111/200\n",
      "5219/5219 [==============================] - 23s 4ms/step - loss: 2.1408 - acc: 0.4242\n",
      "Epoch 112/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 2.1344 - acc: 0.4229\n",
      "Epoch 113/200\n",
      "5219/5219 [==============================] - 28s 5ms/step - loss: 2.1130 - acc: 0.4263\n",
      "Epoch 114/200\n",
      "5219/5219 [==============================] - 33s 6ms/step - loss: 2.0782 - acc: 0.4403\n",
      "Epoch 115/200\n",
      "5219/5219 [==============================] - 31s 6ms/step - loss: 2.0586 - acc: 0.4409\n",
      "Epoch 116/200\n",
      "5219/5219 [==============================] - 32s 6ms/step - loss: 2.0295 - acc: 0.4597\n",
      "Epoch 117/200\n",
      "5219/5219 [==============================] - 29s 6ms/step - loss: 2.0106 - acc: 0.4568\n",
      "Epoch 118/200\n",
      "5219/5219 [==============================] - 29s 5ms/step - loss: 1.9917 - acc: 0.4614\n",
      "Epoch 119/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.9598 - acc: 0.4715\n",
      "Epoch 120/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.9493 - acc: 0.4804\n",
      "Epoch 121/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.9356 - acc: 0.4742\n",
      "Epoch 122/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.9006 - acc: 0.4915\n",
      "Epoch 123/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.8742 - acc: 0.4951\n",
      "Epoch 124/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.8622 - acc: 0.4995\n",
      "Epoch 125/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.8500 - acc: 0.4957\n",
      "Epoch 126/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.8303 - acc: 0.5032\n",
      "Epoch 127/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.7997 - acc: 0.5204\n",
      "Epoch 128/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.7785 - acc: 0.5093\n",
      "Epoch 129/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.7620 - acc: 0.5246\n",
      "Epoch 130/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.7575 - acc: 0.5237\n",
      "Epoch 131/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.7402 - acc: 0.5321\n",
      "Epoch 132/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.7097 - acc: 0.5365\n",
      "Epoch 133/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.6749 - acc: 0.5537\n",
      "Epoch 134/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.6521 - acc: 0.5526\n",
      "Epoch 135/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.6300 - acc: 0.5643\n",
      "Epoch 136/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.6206 - acc: 0.5735\n",
      "Epoch 137/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.5906 - acc: 0.5723\n",
      "Epoch 138/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.5876 - acc: 0.5664\n",
      "Epoch 139/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.5666 - acc: 0.5794\n",
      "Epoch 140/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.5424 - acc: 0.5925\n",
      "Epoch 141/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.5319 - acc: 0.5913\n",
      "Epoch 142/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.5163 - acc: 0.5932\n",
      "Epoch 143/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.4921 - acc: 0.5990\n",
      "Epoch 144/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.4680 - acc: 0.6087\n",
      "Epoch 145/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.4540 - acc: 0.6151\n",
      "Epoch 146/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.4371 - acc: 0.6227\n",
      "Epoch 147/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.4284 - acc: 0.6162\n",
      "Epoch 148/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.4083 - acc: 0.6237\n",
      "Epoch 149/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.3997 - acc: 0.6312\n",
      "Epoch 150/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.3797 - acc: 0.6356\n",
      "Epoch 151/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.3603 - acc: 0.6421\n",
      "Epoch 152/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.3466 - acc: 0.6440\n",
      "Epoch 153/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.3302 - acc: 0.6455\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.3191 - acc: 0.6543\n",
      "Epoch 155/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.3068 - acc: 0.6557\n",
      "Epoch 156/200\n",
      "5219/5219 [==============================] - 27s 5ms/step - loss: 1.3109 - acc: 0.6511\n",
      "Epoch 157/200\n",
      "5219/5219 [==============================] - 28s 5ms/step - loss: 1.2858 - acc: 0.6632\n",
      "Epoch 158/200\n",
      "5219/5219 [==============================] - 26s 5ms/step - loss: 1.2455 - acc: 0.6769\n",
      "Epoch 159/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.2157 - acc: 0.6883\n",
      "Epoch 160/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.1998 - acc: 0.6929\n",
      "Epoch 161/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.1896 - acc: 0.6932\n",
      "Epoch 162/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 1.1823 - acc: 0.6953\n",
      "Epoch 163/200\n",
      "5219/5219 [==============================] - 34s 7ms/step - loss: 1.1702 - acc: 0.6988\n",
      "Epoch 164/200\n",
      "5219/5219 [==============================] - 34s 7ms/step - loss: 1.1527 - acc: 0.7022\n",
      "Epoch 165/200\n",
      "5219/5219 [==============================] - 34s 7ms/step - loss: 1.1379 - acc: 0.7088\n",
      "Epoch 166/200\n",
      "5219/5219 [==============================] - 31s 6ms/step - loss: 1.1257 - acc: 0.7101\n",
      "Epoch 167/200\n",
      "5219/5219 [==============================] - 28s 5ms/step - loss: 1.1100 - acc: 0.7143\n",
      "Epoch 168/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.1003 - acc: 0.7216\n",
      "Epoch 169/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0880 - acc: 0.7220\n",
      "Epoch 170/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0710 - acc: 0.7316\n",
      "Epoch 171/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0525 - acc: 0.7369\n",
      "Epoch 172/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0348 - acc: 0.7329\n",
      "Epoch 173/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0285 - acc: 0.7431\n",
      "Epoch 174/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 1.0107 - acc: 0.7465\n",
      "Epoch 175/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9953 - acc: 0.7551\n",
      "Epoch 176/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9878 - acc: 0.7482\n",
      "Epoch 177/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9869 - acc: 0.7547\n",
      "Epoch 178/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9681 - acc: 0.7580\n",
      "Epoch 179/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9482 - acc: 0.7682\n",
      "Epoch 180/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 0.9326 - acc: 0.7668\n",
      "Epoch 181/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.9096 - acc: 0.7762\n",
      "Epoch 182/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8963 - acc: 0.7831\n",
      "Epoch 183/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 0.8973 - acc: 0.7798\n",
      "Epoch 184/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8943 - acc: 0.7743\n",
      "Epoch 185/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8754 - acc: 0.7820\n",
      "Epoch 186/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8661 - acc: 0.7885\n",
      "Epoch 187/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8499 - acc: 0.7988\n",
      "Epoch 188/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8441 - acc: 0.7979\n",
      "Epoch 189/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8322 - acc: 0.8025\n",
      "Epoch 190/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8036 - acc: 0.8132\n",
      "Epoch 191/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.8094 - acc: 0.8115\n",
      "Epoch 192/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7993 - acc: 0.8076\n",
      "Epoch 193/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7940 - acc: 0.8111\n",
      "Epoch 194/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7711 - acc: 0.8178\n",
      "Epoch 195/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7545 - acc: 0.8253\n",
      "Epoch 196/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7392 - acc: 0.8297\n",
      "Epoch 197/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7348 - acc: 0.8279\n",
      "Epoch 198/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7240 - acc: 0.8289\n",
      "Epoch 199/200\n",
      "5219/5219 [==============================] - 24s 5ms/step - loss: 0.7272 - acc: 0.8276\n",
      "Epoch 200/200\n",
      "5219/5219 [==============================] - 25s 5ms/step - loss: 0.7067 - acc: 0.8379\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd8f8488ac8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=200)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('model.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# load cleaned text sequences\n",
    "in_filename = 'mle/sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1\n",
    "\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "\n",
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready for russian analytica. while facebook had publicly declared itself ready for russian analytica. while facebook had publicly declared itself ready for russian analytica. while facebook had publicly declared itself ready for russian analytica. while facebook had publicly declared itself ready for russian analytica. while facebook had publicly declared itself\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "\n",
    "# generate new text\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b51a4d21630717e92b9f7a23eb53fc6c929bd159"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b51a4d21630717e92b9f7a23eb53fc6c929bd159"
   },
   "outputs": [],
   "source": [
    "plot_model(model,'model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
